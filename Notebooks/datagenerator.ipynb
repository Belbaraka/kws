{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attempt in thisto create a vanilla end to end ASR model with CTC loss using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.insert(1, os.path.abspath('../baseline'))\n",
    "import data_preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep = data_preparation.DataGenarator(path2data=os.path.abspath('../../Datasets/tedlium/TEDLIUM_release1'), \n",
    "                                           path2features=os.path.abspath('../../Datasets/dev_mfcc'), n_mfcc=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading STM file list: 100%|██████████| 8/8 [00:00<00:00, 257.54it/s]\n",
      "saving results: 100%|██████████| 591/591 [01:07<00:00,  8.81it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_file = open(\"metainfo.csv\", \"w\")\n",
    "data_prep.process_tedelium(csv_file, category='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_feats = np.load('/aimlx/Datasets/dev_mfcc/AlGore_2009.sph.wav-13.04.npy', allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 326)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Dense, SimpleRNN, LSTM, CuDNNLSTM, Bidirectional, TimeDistributed, Conv1D, ZeroPadding1D\n",
    "from keras.layers import Lambda, Input, Dropout, Masking\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda implementation of CTC loss, using ctc_batch_cost from TensorFlow backend\n",
    "# CTC implementation from Keras example found at https://github.com/keras-team/keras/blob/master/examples/image_ocr.py\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage:\n",
    "    # print \"y_pred_shape: \", y_pred.shape\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    # print \"y_pred_shape: \", y_pred.shape\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "\n",
    "# Returns clipped relu, clip value set to 20.\n",
    "def clipped_relu(value):\n",
    "    return K.relu(value, max_value=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_rnn(units, input_dim=26, output_dim=29, dropout=0.2, numb_of_dense=3, n_layers=3):\n",
    "    \"\"\"\n",
    "    :param units: Hidden units per layer\n",
    "    :param input_dim: Size of input dimension (number of features), default=26\n",
    "    :param output_dim: Output dim of final layer of model (input to CTC layer), default=29\n",
    "    :param dropout: Dropout rate, default=0.2\n",
    "    :param numb_of_dense: Number of fully connected layers before recurrent, default=3\n",
    "    :param n_layers: Number of simple RNN layers, default=3\n",
    "    :return: network_model: deep_rnn\n",
    "    Default model contains:\n",
    "     1 layer of masking\n",
    "     3 layers of fully connected clipped ReLu (DNN) with dropout 20 % between each layer\n",
    "     3 layers of RNN with 20% dropout\n",
    "     1 layers of fully connected clipped ReLu (DNN) with dropout 20 % between each layer\n",
    "     1 layer of softmax\n",
    "    \"\"\"\n",
    "\n",
    "    # Input data type\n",
    "    dtype = 'float32'\n",
    "\n",
    "    # Kernel and bias initializers for fully connected dense layers\n",
    "    kernel_init_dense = 'random_normal'\n",
    "    bias_init_dense = 'random_normal'\n",
    "\n",
    "    # Kernel and bias initializers for recurrent layer\n",
    "    kernel_init_rnn = 'glorot_uniform'\n",
    "    bias_init_rnn = 'zeros'\n",
    "\n",
    "    # ---- Network model ----\n",
    "    # x_input layer, dim: (batch_size * x_seq_size * mfcc_features)\n",
    "    input_data = Input(name='the_input',shape=(None, input_dim), dtype=dtype)\n",
    "\n",
    "    # Masking layer\n",
    "    x = Masking(mask_value=0., name='masking')(input_data)\n",
    "\n",
    "    # Default 3 fully connected layers DNN ReLu\n",
    "    # Default dropout rate 20 % at each FC layer\n",
    "    for i in range(0, numb_of_dense):\n",
    "        x = TimeDistributed(Dense(units=units, kernel_initializer=kernel_init_dense, bias_initializer=bias_init_dense,\n",
    "                                  activation=clipped_relu), name='fc_'+str(i+1))(x)\n",
    "        x = TimeDistributed(Dropout(dropout), name='dropout_'+str(i+1))(x)\n",
    "\n",
    "    # Deep RNN network with a default of 3 layers\n",
    "    for i in range(0, n_layers):\n",
    "        x = SimpleRNN(units, activation='relu', kernel_initializer=kernel_init_rnn, bias_initializer=bias_init_rnn,\n",
    "                      dropout=dropout, return_sequences=True, name=('deep_rnn_'+ str(i+1)))(x)\n",
    "\n",
    "    # 1 fully connected layer DNN ReLu with default 20% dropout\n",
    "    x = TimeDistributed(Dense(units=units, kernel_initializer=kernel_init_dense, bias_initializer=bias_init_dense,\n",
    "                              activation='relu'), name='fc_4')(x)\n",
    "    x = TimeDistributed(Dropout(dropout), name='dropout_4')(x)\n",
    "\n",
    "    # Output layer with softmax\n",
    "    y_pred = TimeDistributed(Dense(units=output_dim, kernel_initializer=kernel_init_dense,\n",
    "                                   bias_initializer=bias_init_dense, activation='softmax'), name='softmax')(x)\n",
    "\n",
    "    # ---- CTC ----\n",
    "    # y_input layers (transcription data) for CTC loss\n",
    "    labels = Input(name='the_labels', shape=[None], dtype=dtype)        # transcription data (batch_size * y_seq_size)\n",
    "    input_length = Input(name='input_length', shape=[1], dtype=dtype)   # unpadded len of all x_sequences in batch\n",
    "    label_length = Input(name='label_length', shape=[1], dtype=dtype)   # unpadded len of all y_sequences in batch\n",
    "\n",
    "    # Lambda layer with ctc_loss function due to Keras not supporting CTC layers\n",
    "    loss_out = Lambda(function=ctc_lambda_func, name='ctc', output_shape=(1,))(\n",
    "                      [y_pred, labels, input_length, label_length])\n",
    "\n",
    "    network_model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)\n",
    "\n",
    "    return network_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
