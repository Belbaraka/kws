{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attempt in thisto create an end to end ASR model with CTC loss using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.insert(1, os.path.abspath('../models'))\n",
    "import data_preparation \n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2data = os.path.abspath('../../Datasets/tedlium/TEDLIUM_release1')\n",
    "path2features = os.path.abspath('../../Datasets/train_mfcc')\n",
    "filename = 'ids_labels.p'\n",
    "n_mfcc = 26\n",
    "n_mels = 40\n",
    "hop_length = 160\n",
    "frame_length = 320\n",
    "batch_size = 32\n",
    "epoch_length = 32\n",
    "shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(os.path.join(path2features, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>nb_frames</th>\n",
       "      <th>labels</th>\n",
       "      <th>offset</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [filename, nb_frames, labels, offset, duration]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_params = {'path2data': path2data,\n",
    "                    'path2features': path2features,\n",
    "                    'pickle_filename': filename,\n",
    "                    'n_mfcc': n_mfcc,\n",
    "                    'n_mels': n_mels,\n",
    "                    'hop_length': hop_length,\n",
    "                    'frame_length': frame_length\n",
    "                    }\n",
    "\n",
    "data_prep = data_preparation.DataPrep(**data_prep_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading STM file list: 100%|██████████| 774/774 [00:03<00:00, 230.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tasks to be performed :  56803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=11)]: Using backend LokyBackend with 11 concurrent workers.\n",
      "[Parallel(n_jobs=11)]: Done  28 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=11)]: Done 178 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=11)]: Done 428 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=11)]: Done 778 tasks      | elapsed:   31.5s\n",
      "[Parallel(n_jobs=11)]: Done 1228 tasks      | elapsed:   49.1s\n",
      "[Parallel(n_jobs=11)]: Done 1778 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=11)]: Done 2428 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=11)]: Done 3178 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=11)]: Done 4028 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=11)]: Done 4978 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=11)]: Done 6028 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=11)]: Done 7178 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=11)]: Done 8661 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=11)]: Done 10423 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=11)]: Done 11873 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=11)]: Done 13423 tasks      | elapsed:  7.9min\n",
      "[Parallel(n_jobs=11)]: Done 15073 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=11)]: Done 16876 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=11)]: Done 20040 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=11)]: Done 21990 tasks      | elapsed: 13.2min\n"
     ]
    }
   ],
   "source": [
    "data_prep.process_tedelium(category='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator_params = {'path2features': path2features,\n",
    "                         'pickle_filename': filename,\n",
    "                         'batch_size': batch_size,\n",
    "                         'mfcc_features': n_mfcc,\n",
    "                         'epoch_length': epoch_length,\n",
    "                         'shuffle': shuffle\n",
    "                        }\n",
    "\n",
    "data_gen = data_preparation.DataGenerator(**data_generator_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Dense, SimpleRNN, LSTM, CuDNNLSTM, Bidirectional, TimeDistributed, Conv1D, ZeroPadding1D, GRU\n",
    "from keras.layers import Lambda, Input, Dropout, Masking\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda implementation of CTC loss, using ctc_batch_cost from TensorFlow backend\n",
    "# CTC implementation from Keras example found at https://github.com/keras-team/keras/blob/master/examples/image_ocr.py\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage:\n",
    "    # print \"y_pred_shape: \", y_pred.shape\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    # print \"y_pred_shape: \", y_pred.shape\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "\n",
    "# Returns clipped relu, clip value set to 20.\n",
    "def clipped_relu(value):\n",
    "    return K.relu(value, max_value=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model(units, input_dim=26, output_dim=29):\n",
    "    \n",
    "    dtype = 'float32'\n",
    "    \n",
    "    # Kernel and bias initializers for fully connected dense layers\n",
    "    kernel_init_dense = 'random_normal'\n",
    "    bias_init_dense = 'random_normal'\n",
    "    \n",
    "    input_data = Input(name='the_input',shape=(None, input_dim), dtype=dtype)\n",
    "    \n",
    "    x = GRU(units, activation='tanh', return_sequences=True)(input_data)\n",
    "    x = GRU(units, activation='tanh', return_sequences=True, dropout=0.3)(x)\n",
    "    \n",
    "    # Output layer with softmax\n",
    "    y_pred = TimeDistributed(Dense(units=output_dim, kernel_initializer=kernel_init_dense,\n",
    "                                   bias_initializer=bias_init_dense, activation='softmax'), name='softmax')(x)\n",
    "\n",
    "    # ---- CTC ----\n",
    "    # y_input layers (transcription data) for CTC loss\n",
    "    labels = Input(name='the_labels', shape=[None], dtype=dtype)        # transcription data (batch_size * y_seq_size)\n",
    "    input_length = Input(name='input_length', shape=[1], dtype=dtype)   # unpadded len of all x_sequences in batch\n",
    "    label_length = Input(name='label_length', shape=[1], dtype=dtype)   # unpadded len of all y_sequences in batch\n",
    "\n",
    "    # Lambda layer with ctc_loss function due to Keras not supporting CTC layers\n",
    "    loss_out = Lambda(function=ctc_lambda_func, name='ctc', output_shape=(1,))(\n",
    "                      [y_pred, labels, input_length, label_length])\n",
    "\n",
    "    network_model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)\n",
    "    \n",
    "    return network_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_rnn(units, input_dim=26, output_dim=29, dropout=0.2, numb_of_dense=1, n_layers=1):\n",
    "    \"\"\"\n",
    "    :param units: Hidden units per layer\n",
    "    :param input_dim: Size of input dimension (number of features), default=26\n",
    "    :param output_dim: Output dim of final layer of model (input to CTC layer), default=29\n",
    "    :param dropout: Dropout rate, default=0.2\n",
    "    :param numb_of_dense: Number of fully connected layers before recurrent, default=3\n",
    "    :param n_layers: Number of simple RNN layers, default=3\n",
    "    :return: network_model: deep_rnn\n",
    "    Default model contains:\n",
    "     1 layer of masking\n",
    "     3 layers of fully connected clipped ReLu (DNN) with dropout 20 % between each layer\n",
    "     3 layers of RNN with 20% dropout\n",
    "     1 layers of fully connected clipped ReLu (DNN) with dropout 20 % between each layer\n",
    "     1 layer of softmax\n",
    "    \"\"\"\n",
    "\n",
    "    # Input data type\n",
    "    dtype = 'float32'\n",
    "\n",
    "    # Kernel and bias initializers for fully connected dense layers\n",
    "    kernel_init_dense = 'random_normal'\n",
    "    bias_init_dense = 'random_normal'\n",
    "\n",
    "    # Kernel and bias initializers for recurrent layer\n",
    "    kernel_init_rnn = 'glorot_uniform'\n",
    "    bias_init_rnn = 'zeros'\n",
    "\n",
    "    # ---- Network model ----\n",
    "    # x_input layer, dim: (batch_size * x_seq_size * mfcc_features)\n",
    "    input_data = Input(name='the_input',shape=(None, input_dim), dtype=dtype)\n",
    "\n",
    "    # Masking layer\n",
    "    x = Masking(mask_value=0., name='masking')(input_data)\n",
    "\n",
    "    # Default 3 fully connected layers DNN ReLu\n",
    "    # Default dropout rate 20 % at each FC layer\n",
    "    for i in range(0, numb_of_dense):\n",
    "        x = TimeDistributed(Dense(units=units, kernel_initializer=kernel_init_dense, bias_initializer=bias_init_dense,\n",
    "                                  activation=clipped_relu), name='fc_'+str(i+1))(x)\n",
    "        x = TimeDistributed(Dropout(dropout), name='dropout_'+str(i+1))(x)\n",
    "\n",
    "    # Deep RNN network with a default of 3 layers\n",
    "    for i in range(0, n_layers):\n",
    "        x = SimpleRNN(units, activation='relu', kernel_initializer=kernel_init_rnn, bias_initializer=bias_init_rnn,\n",
    "                      dropout=dropout, return_sequences=True, name=('deep_rnn_'+ str(i+1)))(x)\n",
    "\n",
    "    # 1 fully connected layer DNN ReLu with default 20% dropout\n",
    "    x = TimeDistributed(Dense(units=units, kernel_initializer=kernel_init_dense, bias_initializer=bias_init_dense,\n",
    "                              activation='relu'), name='fc_' + str(numb_of_dense + 1))(x)\n",
    "    x = TimeDistributed(Dropout(dropout), name='dropout_' + str(numb_of_dense + 1))(x)\n",
    "\n",
    "    # Output layer with softmax\n",
    "    y_pred = TimeDistributed(Dense(units=output_dim, kernel_initializer=kernel_init_dense,\n",
    "                                   bias_initializer=bias_init_dense, activation='softmax'), name='softmax')(x)\n",
    "\n",
    "    # ---- CTC ----\n",
    "    # y_input layers (transcription data) for CTC loss\n",
    "    labels = Input(name='the_labels', shape=[None], dtype=dtype)        # transcription data (batch_size * y_seq_size)\n",
    "    input_length = Input(name='input_length', shape=[1], dtype=dtype)   # unpadded len of all x_sequences in batch\n",
    "    label_length = Input(name='label_length', shape=[1], dtype=dtype)   # unpadded len of all y_sequences in batch\n",
    "\n",
    "    # Lambda layer with ctc_loss function due to Keras not supporting CTC layers\n",
    "    loss_out = Lambda(function=ctc_lambda_func, name='ctc', output_shape=(1,))(\n",
    "                      [y_pred, labels, input_length, label_length])\n",
    "\n",
    "    network_model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)\n",
    "\n",
    "    return network_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blstm(units, input_dim=26, output_dim=29, dropout=0.2, numb_of_dense=3, cudnn=False, n_layers=1):\n",
    "    \"\"\"\n",
    "    :param units: Hidden units per layer\n",
    "    :param input_dim: Size of input dimension (number of features), default=26\n",
    "    :param output_dim: Output dim of final layer of model (input to CTC layer), default=29\n",
    "    :param dropout: Dropout rate, default=0.2\n",
    "    :param numb_of_dense: Number of fully connected layers before recurrent, default=3\n",
    "    :param cudnn: Whether to use the CuDNN optimized LSTM (GPU only), default=False\n",
    "    :param n_layers: Number of stacked BLSTM layers, default=1\n",
    "    :return: network_model: blstm\n",
    "    Default model contains:\n",
    "     1 layer of masking\n",
    "     3 layers of fully connected clipped ReLu (DNN) with dropout 20 % between each layer\n",
    "     1 layer of BLSTM\n",
    "     1 layers of fully connected clipped ReLu (DNN) with dropout 20 % between each layer\n",
    "     1 layer of softmax\n",
    "    \"\"\"\n",
    "\n",
    "    # Input data type\n",
    "    dtype = 'float32'\n",
    "\n",
    "    # Kernel and bias initializers for fully connected dense layers\n",
    "    kernel_init_dense = 'random_normal'\n",
    "    bias_init_dense = 'random_normal'\n",
    "\n",
    "    # Kernel and bias initializers for recurrent layer\n",
    "    kernel_init_rnn = 'glorot_uniform'\n",
    "    bias_init_rnn = 'random_normal'\n",
    "\n",
    "    # ---- Network model ----\n",
    "    # x_input layer, dim: (batch_size * x_seq_size * features)\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim), dtype=dtype)\n",
    "\n",
    "    if cudnn:\n",
    "        # CuDNNLSTM does not support masking\n",
    "        x = input_data\n",
    "    else:\n",
    "        # Masking layer\n",
    "        x = Masking(mask_value=0., name='masking')(input_data)\n",
    "\n",
    "    # Default 3 fully connected layers DNN ReLu\n",
    "    # Default dropout rate 20 % at each FC layer\n",
    "    for i in range(0, numb_of_dense):\n",
    "        x = TimeDistributed(Dense(units=units, kernel_initializer=kernel_init_dense, bias_initializer=bias_init_dense,\n",
    "                                  activation=clipped_relu), name='fc_'+str(i+1))(x)\n",
    "        x = TimeDistributed(Dropout(dropout), name='dropout_'+str(i+1))(x)\n",
    "\n",
    "    # Bidirectional RNN (with ReLu)\n",
    "    # If running on GPU, use the CuDNN optimised LSTM model\n",
    "    if cudnn:\n",
    "        for i in range(0, n_layers):\n",
    "            x = Bidirectional(CuDNNLSTM(units, kernel_initializer=kernel_init_rnn, bias_initializer=bias_init_rnn,\n",
    "                                        unit_forget_bias=True, return_sequences=True),\n",
    "                              merge_mode='sum', name=('CuDNN_bi_lstm' + str(i+1)))(x)\n",
    "    else:\n",
    "        for i in range(0, n_layers):\n",
    "            x = Bidirectional(LSTM(units, activation='relu', kernel_initializer=kernel_init_rnn, dropout=dropout,\n",
    "                                   bias_initializer=bias_init_rnn, return_sequences=True),\n",
    "                              merge_mode='sum', name=('bi_lstm' + str(i+1)))(x)\n",
    "\n",
    "    # 1 fully connected layer DNN ReLu with default 20% dropout\n",
    "    x = TimeDistributed(Dense(units=units, kernel_initializer=kernel_init_dense, bias_initializer=bias_init_dense,\n",
    "                              activation='relu'), name='fc_4')(x)\n",
    "    x = TimeDistributed(Dropout(dropout), name='dropout_4')(x)\n",
    "\n",
    "    # Output layer with softmax\n",
    "    y_pred = TimeDistributed(Dense(units=output_dim, kernel_initializer=kernel_init_dense,\n",
    "                                   bias_initializer=bias_init_dense, activation='softmax'), name='softmax')(x)\n",
    "\n",
    "    # ---- CTC ----\n",
    "    # y_input layers (transcription data) for CTC loss\n",
    "    labels = Input(name='the_labels', shape=[None], dtype=dtype)       # transcription data (batch_size * y_seq_size)\n",
    "    input_length = Input(name='input_length', shape=[1], dtype=dtype)  # unpadded len of all x_sequences in batch\n",
    "    label_length = Input(name='label_length', shape=[1], dtype=dtype)  # unpadded len of all y_sequences in batch\n",
    "\n",
    "    # Lambda layer with ctc_loss function due to Keras not supporting CTC layers\n",
    "    loss_out = Lambda(function=ctc_lambda_func, name='ctc', output_shape=(1,))(\n",
    "                      [y_pred, labels, input_length, label_length])\n",
    "\n",
    "    network_model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)\n",
    "\n",
    "    return network_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_blstm(units, input_dim=26, output_dim=29, dropout=0.2, seq_padding=2176, cudnn=False, n_layers=1):\n",
    "    \"\"\"\n",
    "    :param units: Hidden units per layer\n",
    "    :param input_dim: Size of input dimension (number of features), default=26\n",
    "    :param output_dim: Output dim of final layer of model (input to CTC layer), default=29\n",
    "    :param dropout: Dropout rate, default=0.2\n",
    "    :param seq_padding: length of sequence zero padding before conv layers, default=2176\n",
    "    :param cudnn: Whether to use the CuDNN optimized LSTM (only for GPU), default=False\n",
    "    :param n_layers: Number of stacked BLSTM layers, default=1\n",
    "    :return: network_model: cnn_blstm\n",
    "    Model contains:\n",
    "     3 layers of CNN Conv1D\n",
    "     3 layers of BLSTM\n",
    "     1 layers of fully connected clipped ReLu (DNN) with dropout 20 % between each layer\n",
    "     1 layer of softmax\n",
    "    \"\"\"\n",
    "\n",
    "    # Input data type\n",
    "    dtype = 'float32'\n",
    "\n",
    "    activation_conv = clipped_relu\n",
    "\n",
    "    # Kernel and bias initializers for fully connected dense layer\n",
    "    kernel_init_dense = 'random_normal'\n",
    "    bias_init_dense = 'random_normal'\n",
    "\n",
    "    # Kernel and bias initializers for convolution layers\n",
    "    kernel_init_conv = 'glorot_uniform'\n",
    "    bias_init_conv = 'random_normal'\n",
    "\n",
    "    # Kernel and bias initializers for recurrent layer\n",
    "    kernel_init_rnn = 'glorot_uniform'\n",
    "    bias_init_rnn = 'random_normal'\n",
    "\n",
    "    # ---- Network model ----\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim), dtype=dtype)\n",
    "\n",
    "    # Pad on sequence dim so all sequences are equal length\n",
    "    x = ZeroPadding1D(padding=(0, seq_padding))(input_data)\n",
    "\n",
    "    # 3 x 1D convolutional layers with strides: 1, 1, 2\n",
    "    x = Conv1D(filters=units, kernel_size=5, strides=1, activation=activation_conv,\n",
    "               kernel_initializer=kernel_init_conv, bias_initializer=bias_init_conv, name='conv_1')(x)\n",
    "    x = TimeDistributed(Dropout(dropout), name='dropout_1')(x)\n",
    "\n",
    "    x = Conv1D(filters=units, kernel_size=5, strides=1, activation=activation_conv,\n",
    "               kernel_initializer=kernel_init_conv, bias_initializer=bias_init_conv, name='conv_2')(x)\n",
    "    x = TimeDistributed(Dropout(dropout), name='dropout_2')(x)\n",
    "\n",
    "    x = Conv1D(filters=units, kernel_size=5, strides=2, activation=activation_conv,\n",
    "               kernel_initializer=kernel_init_conv, bias_initializer=bias_init_conv, name='conv_3')(x)\n",
    "    x = TimeDistributed(Dropout(dropout), name='dropout_3')(x)\n",
    "\n",
    "    # Bidirectional LSTM\n",
    "    if cudnn:\n",
    "        for i in range(0, n_layers):\n",
    "            x = Bidirectional(CuDNNLSTM(units, kernel_initializer=kernel_init_rnn, bias_initializer=bias_init_rnn,\n",
    "                                        unit_forget_bias=True, return_sequences=True),\n",
    "                              merge_mode='sum', name='CuDNN_bi_lstm'+str(i+1))(x)\n",
    "    else:\n",
    "        for i in range(0, n_layers):\n",
    "            x = Bidirectional(LSTM(units, activation='relu', kernel_initializer=kernel_init_rnn, dropout=dropout,\n",
    "                                   bias_initializer=bias_init_rnn, return_sequences=True),\n",
    "                              merge_mode='sum', name='bi_lstm'+str(i+1))(x)\n",
    "\n",
    "    # 1 fully connected layer DNN ReLu with default 20% dropout\n",
    "    x = TimeDistributed(Dense(units=units, kernel_initializer=kernel_init_dense, bias_initializer=bias_init_dense,\n",
    "                              activation='relu'), name='fc_4')(x)\n",
    "    x = TimeDistributed(Dropout(dropout), name='dropout_4')(x)\n",
    "\n",
    "    # Output layer with softmax\n",
    "    y_pred = TimeDistributed(Dense(units=output_dim, kernel_initializer=kernel_init_dense,\n",
    "                                   bias_initializer=bias_init_dense, activation='softmax'), name='softmax')(x)\n",
    "\n",
    "    # ---- CTC ----\n",
    "    # y_input layers (transcription data) for CTC loss\n",
    "    labels = Input(name='the_labels', shape=[None], dtype=dtype)       # transcription data (batch_size * y_seq_size)\n",
    "    input_length = Input(name='input_length', shape=[1], dtype=dtype)  # unpadded len of all x_sequences in batch\n",
    "    label_length = Input(name='label_length', shape=[1], dtype=dtype)  # unpadded len of all y_sequences in batch\n",
    "\n",
    "    # Lambda layer with ctc_loss function due to Keras not supporting CTC layers\n",
    "    loss_out = Lambda(function=ctc_lambda_func, name='ctc', output_shape=(1,))(\n",
    "                      [y_pred, labels, input_length, label_length])\n",
    "\n",
    "    network_model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)\n",
    "\n",
    "    return network_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "15/15 [==============================] - 50s 3s/step - loss: 733.6994\n",
      "Epoch 2/6\n",
      "15/15 [==============================] - 47s 3s/step - loss: 576.0746\n",
      "Epoch 3/6\n",
      "15/15 [==============================] - 47s 3s/step - loss: 551.3177\n",
      "Epoch 4/6\n",
      "15/15 [==============================] - 47s 3s/step - loss: 546.0519\n",
      "Epoch 5/6\n",
      "15/15 [==============================] - 47s 3s/step - loss: 542.9732\n",
      "Epoch 6/6\n",
      "15/15 [==============================] - 47s 3s/step - loss: 542.9252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f98b341ceb8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = {'ctc': lambda y_true, y_pred: y_pred}\n",
    "training_generator = data_gen\n",
    "model = deep_rnn(units=256, input_dim=n_mfcc, output_dim=29, dropout=0.2, numb_of_dense=3, n_layers=3)\n",
    "#model = blstm(units=256, input_dim=n_mfcc, output_dim=29, dropout=0.2, numb_of_dense=1, cudnn=False, n_layers=1)\n",
    "#model = base_model(units=64, input_dim=n_mfcc, output_dim=29)\n",
    "model_train_params = {'generator': training_generator,\n",
    "                      'epochs': 6,\n",
    "                      'verbose': 1,\n",
    "                      #'validation_data': validation_generator,\n",
    "                      'workers': -1,\n",
    "                      'shuffle': shuffle}\n",
    "\n",
    "optimizer = Adam(lr=0.0001, epsilon=1e-8, clipnorm=2.0)\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit_generator(**model_train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "15/15 [==============================] - 28s 2s/step - loss: 543.2368\n",
      "Epoch 2/6\n",
      "15/15 [==============================] - 28s 2s/step - loss: 542.5503\n",
      "Epoch 3/6\n",
      "15/15 [==============================] - 28s 2s/step - loss: 541.6669\n",
      "Epoch 4/6\n",
      "15/15 [==============================] - 28s 2s/step - loss: 541.6401\n",
      "Epoch 5/6\n",
      "15/15 [==============================] - 28s 2s/step - loss: 540.6022\n",
      "Epoch 6/6\n",
      "15/15 [==============================] - 28s 2s/step - loss: 540.6746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbe986920f0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(**model_train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "15/15 [==============================] - 29s 2s/step - loss: 544.8704\n",
      "Epoch 2/6\n",
      "15/15 [==============================] - 29s 2s/step - loss: 543.1136\n",
      "Epoch 3/6\n",
      "15/15 [==============================] - 29s 2s/step - loss: 542.5406\n",
      "Epoch 4/6\n",
      "15/15 [==============================] - 29s 2s/step - loss: 539.6143\n",
      "Epoch 5/6\n",
      "15/15 [==============================] - 29s 2s/step - loss: 536.9920\n",
      "Epoch 6/6\n",
      "15/15 [==============================] - 29s 2s/step - loss: 533.9092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4079295e80>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(**model_train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, None, 26)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, None, 26)     0           the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "fc_1 (TimeDistributed)          (None, None, 256)    6912        masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (TimeDistributed)     (None, None, 256)    0           fc_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_2 (TimeDistributed)          (None, None, 256)    65792       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (TimeDistributed)     (None, None, 256)    0           fc_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_3 (TimeDistributed)          (None, None, 256)    65792       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (TimeDistributed)     (None, None, 256)    0           fc_3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "deep_rnn_1 (SimpleRNN)          (None, None, 256)    131328      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "deep_rnn_2 (SimpleRNN)          (None, None, 256)    131328      deep_rnn_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "deep_rnn_3 (SimpleRNN)          (None, None, 256)    131328      deep_rnn_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_4 (TimeDistributed)          (None, None, 256)    65792       deep_rnn_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (TimeDistributed)     (None, None, 256)    0           fc_4[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "softmax (TimeDistributed)       (None, None, 29)     7453        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           softmax[0][0]                    \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 605,725\n",
      "Trainable params: 605,725\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a test function that takes preprocessed sound input and outputs predictions\n",
    "# Used to calculate WER (word error rate) while training the network\n",
    "input_data = model.get_layer('the_input').input\n",
    "y_pred = model.get_layer('ctc').input[0]\n",
    "test_func = K.function([input_data], [y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_decode(test_func, x_data):\n",
    "    \"\"\"\n",
    "    Calculate network probabilities with test_func and decode with max decode/greedy decode\n",
    "    :param test_func: Keras function that takes preprocessed audio input and outputs network predictions\n",
    "    :param x_data: preprocessed audio data\n",
    "    :return: decoded: max decoded network output\n",
    "    \"\"\"\n",
    "    y_pred = test_func([x_data])[0]\n",
    "    decoded = []\n",
    "    print(y_pred.shape)\n",
    "    for i in range(0, y_pred.shape[0]):\n",
    "\n",
    "        decoded_batch = []\n",
    "        for j in range(0,y_pred.shape[1]):\n",
    "            decoded_batch.append(np.argmax(y_pred[i][j][:-1]))\n",
    "        #print(decoded_batch)\n",
    "        temp = [k for k, g in groupby(decoded_batch)]\n",
    "        #print(temp)\n",
    "        temp[:] = [x for x in temp if x != [28]]\n",
    "        #print(temp)\n",
    "        decoded.append(temp)\n",
    "\n",
    "    return decoded\n",
    "\n",
    "\n",
    "\n",
    "def predict_on_batch(data_gen, test_func, batch_index):\n",
    "    \"\"\"\n",
    "    Produce a sample of predictions at given batch index from data in data_gen\n",
    "    :param data_gen: DataGenerator to produce input data\n",
    "    :param test_func: Keras function that takes preprocessed audio input and outputs network predictions\n",
    "    :param batch_index: which batch to use as input data\n",
    "    :return: List containing original transcripts and predictions\n",
    "    \"\"\"\n",
    "    input_data, _ = data_gen.__getitem__(batch_index)\n",
    "\n",
    "    x_data = input_data.get(\"the_input\")\n",
    "    y_data = input_data.get(\"the_labels\")\n",
    "\n",
    "    res = max_decode(test_func, x_data)\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(y_data.shape[0]):\n",
    "        original = \"\".join(index2str(y_data[i].astype('int')))\n",
    "        predicted = \"\".join(index2str(res[i]))\n",
    "        predictions.append([original,predicted])\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 726, 29)\n"
     ]
    }
   ],
   "source": [
    "input_data, _ = data_gen.__getitem__(1)\n",
    "\n",
    "x_data = input_data.get(\"the_input\")\n",
    "y_data = input_data.get(\"the_labels\")\n",
    "y_pred = test_func([x_data])[0]\n",
    "predictions = predict_on_batch(data_gen, test_func, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 726, 12)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['it s a waste of money what i m telling you is that these expensive complicated choices it s not simply that they don t help they actually hurt they actually make us worse',\n",
       "  ' '],\n",
       " ['nepal his mother was incarcerated for the price of for the crime of being wealthy he was smuggled into the jail at the ti at the age of two to hide beneath her skirt tails because she couldn t bear to be without him the sister who had done that brave deed was put into an education camp one day she inadvertently stepped on an armband that of mao and for that transgression she was given seven years of hard labor',\n",
       "  ' '],\n",
       " ['we have to design so they can go together we design unique elements into this you may have read that we put watermarks in think of this we have a four letter genetic code a c g and t triplets of that letter of those letters code for roughly twenty amino acids that there s a single letter',\n",
       "  ' '],\n",
       " ['now if you begin to look at the idea that these cultures could create different realities you could begin to understand some of their extraordinary discoveries take this plant here it s a photograph i took in the northwest amazon just last april this is ayahuasca which many of you have heard about the most powerful psychoactive preparation of',\n",
       "  ' '],\n",
       " ['so what this means this incredible freedom of choice we have with respect to work is that we have to make a decision again and again and again about whether',\n",
       "  ' '],\n",
       " ['moonlight dances of sacred dance and music that would go on for hours and hours until dawn and they were always magnificent because the dancers were professionals and they were terrific right but every once in a while very rarely something would happen and one of these performers would actually become transcendent',\n",
       "  ' '],\n",
       " ['turns out all he wanted to do was to stack the siftables up so to him they were just blocks',\n",
       "  ' '],\n",
       " ['it doesn t doesnt make any difference because the only thing that that ought to limit the performance of a system like this one is the number of pixels on your screen at any given moment',\n",
       "  ' '],\n",
       " ['not only would those people s lives be improved but ours would be improved also this is what economists call a pareto improving move income redistribution will make everyone',\n",
       "  ' '],\n",
       " ['and it s doing that based on the content inside the images and that gets really exciting when you think about the richness of the semantic information that a lot of those images have like when you do a web search for images',\n",
       "  ' '],\n",
       " ['here is an example of it there s an organism called deinococcus radiodurans that can take three millions rads of radiation you can see in the top panel its chromosome just gets blown apart',\n",
       "  ' '],\n",
       " ['i m going to talk to you about some stuff that s in this book of mine that i hope will resonate with other things you ve already heard and i ll try to',\n",
       "  ' '],\n",
       " ['cell phone call should i respond to this email should i draft this letter and even if the answer to the question is no it s certainly going to make the experience of your kid s soccer game very different than it would ve been',\n",
       "  ' '],\n",
       " ['they laughed more than you guys are it s', ' '],\n",
       " ['and this was our first attempt starting with the digital information of the genome of phi x one seventy four it s a small virus that kills bacteria we designed the pieces went through our error correction and had a dna molecule of about five thousand letters',\n",
       "  ' '],\n",
       " ['and i ve argued that we re about to perhaps create a new version of the cambrian explosion where there s massive new speciation based on this digital design',\n",
       "  ' '],\n",
       " ['and you know if we think about it this way it starts to change everything you know this is how i ve started to think and this is certainly how i ve been thinking in the last few months you know as i ve been working on the book that will soon be published as the',\n",
       "  ' '],\n",
       " ['so by shaking the siftables and putting them next to each other he can make the characters interact',\n",
       "  ' '],\n",
       " ['i had very low exp i had no particular expectations when they only came in one flavor when they came in a hundred flavors damn it one of them should ve been perfect and what i got was good but it wasn t perfect and so i compared what i got to what i expected',\n",
       "  ' '],\n",
       " ['in other aspects of life that are much more significant than buying things the same explosion of choice is true health care it is no longer the case in the united states that you',\n",
       "  ' '],\n",
       " ['do i do i understand this do i understand this right that what what your software is going to allow is that at some point really within the next few years',\n",
       "  ' '],\n",
       " ['you know what the hell with it i m going to keep writing anyway because that s my job and i would please like the record to reflect today that i showed up for my part of the job',\n",
       "  ' '],\n",
       " ['lhasa that i understood the face behind the statistics you hear about six thousand sacred monuments torn apart to dust and ashes',\n",
       "  ' '],\n",
       " ['better off not just poor people because of how all this excess choice plagues us so to conclude',\n",
       "  ' '],\n",
       " ['that is to say there were some choices but not everything was a matter of choice and the world we now live in looks like this and the question is',\n",
       "  ' '],\n",
       " ['there s somewhere in here there is actually there there is series of photographs here we go this is actually a poster of notre dame that registered correctly here we can dive in from the poster to a physical view',\n",
       "  ' '],\n",
       " ['so the alliance for climate protection has launched two campaigns this is one of them part of one of them',\n",
       "  ' '],\n",
       " ['the thread that links them all our addiction to carbon based fuels like dirty coal and foreign oil',\n",
       "  ' '],\n",
       " ['all of these are consuming questions and they re going to answer these questions whether or not it means not doing all the work i assign and not getting a good grade in my courses and indeed they should these are important questions to answer',\n",
       "  ' '],\n",
       " ['what this is doing', ' '],\n",
       " ['do i look like i can write down a song right now you know if you really want to exist come back at a more opportune moment when i can take care of you otherwise go bother somebody else today go bother leonard cohen you know',\n",
       "  ' '],\n",
       " ['this is an annual melting river but the volumes are much larger than ever this is the kangerlussuaq river in southwest',\n",
       "  ' ']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 2652, 26)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 526)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-160.77806228,   92.01941971,  -23.9410947 ,   18.6731909 ,\n",
       "         -7.71552052,  -10.06911783,  -10.4417597 ,  -10.54981956,\n",
       "         12.89189723,    3.71269653,   -6.12233873,    0.42868154,\n",
       "         -1.12947846,    7.82820096,  -13.72863764,    3.45903755,\n",
       "         -5.76475135,   -2.92914885,    2.75763579,   -3.04279983,\n",
       "         -0.39132807,    0.83946294,   -7.74940364,    2.94824848,\n",
       "         -3.00777463,   -2.28515724])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data[0][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the short answer to all those questions is yes yes i m afraid of all those things and i always have been and i m afraid of many many more things besides that you know people can t even guess at like',\n",
       "  'r'],\n",
       " ['blaise that is truly incredible congratulations', ' r r r'],\n",
       " ['assumption being that sub saharan africa had no religious beliefs well of course they did and voodoo is simply the distillation of these very',\n",
       "  ' r'],\n",
       " ['each of these has four different variations you get to choose which one you want to use and you can inject these sounds into a sequence that you can assemble into the pattern that you want',\n",
       "  ' r r'],\n",
       " ['you will never be pleasantly surprised because your expectations my expectations have gone through the roof the secret to happiness this is what you all came for the secret to happiness is low expectations',\n",
       "  ' r'],\n",
       " ['this is the last one', ' r'],\n",
       " ['thank you so', ' r r r r r r'],\n",
       " ['she s okay the question is whether we will be and one reason is this enormous heat sink heats up greenland from the north',\n",
       "  ' r'],\n",
       " ['and with a very short period of time all the characteristics of one species were lost and it converted totally into the new species based on the new software that we put in the cell',\n",
       "  ' r r r'],\n",
       " ['i i should just put it bluntly because we re all sort of friends here now it s exceedingly likely that my greatest success is behind me',\n",
       "  'r'],\n",
       " ['or one of the most fascinating tribes i ever lived with the waorani of northeastern ecuador an astonishing people first contacted peacefully in nineteen fifty eight in nineteen fifty seven five missionaries attempted contact and made a critical mistake they dropped from the air eight by ten glossy photographs of themselves in what we would say to be friendly gestures forgetting that these people of the rainforest had never seen anything two dimensional in their lives they picked up these photographs from the forest floor',\n",
       "  ' r'],\n",
       " ['kogi and you ll suddenly discover what it would be like to be unable to speak your own language and so what i d like to do with you today is sort of take you',\n",
       "  ' r'],\n",
       " ['it s not at all my creative process i m not the pipeline you know like i m a mule and the way that i have to work is that i have to get up at the same time every day and like sweat and labor and like barrel through it really awkwardly but even i in my mulishness even i have brushed up against that thing',\n",
       "  'r r'],\n",
       " ['one point two million people killed by the cadres during the cultural revolution this young man s father had been ascribed to the panchen lama that meant he was instantly killed at the time of the chinese invasion his uncle fled with his holiness in the diaspora that took the people to',\n",
       "  ' r'],\n",
       " ['like pouring a color the way we might pour a liquid so in this case we ve got three siftables configured to be paint buckets and i can use them to pour color into that central one where they get mixed if we overshoot we can pour a little bit back',\n",
       "  ' r r r r r r r r r r r r r r r r r r r r r'],\n",
       " ['so when i heard that story it started to shift a little bit the way that i worked too and it already saved me once this idea it saved me when i was in the middle of writing eat pray love and i fell into one of those sort of pits of despair that we all fall into when we re working on something and it s not coming and you start to think this is gonna be a disaster this is gonna be the worst book ever written',\n",
       "  ' r'],\n",
       " ['this is an annual melting river but the volumes are much larger than ever this is the kangerlussuaq river in southwest',\n",
       "  ' r'],\n",
       " ['and the result is we call it patient autonomy which makes it sound like a good thing but what it really is is a shifting of the burden and the responsibility for decision making from somebody who knows something namely the doctor to somebody who knows nothing and is almost certainly sick and thus not in the best shape to be making decisions namely the patient there s enormous marketing of prescription drugs',\n",
       "  ' r'],\n",
       " ['i ll give you some examples of what modern progress has made possible for us this is my supermarket not such a big one',\n",
       "  ' r'],\n",
       " ['but this understates the seriousness of this particular problem because it doesn t show the thickness of the ice the arctic ice cap is in a sense the beating heart of the global climate system it expands in winter and contracts in summer the next slide i show you will be',\n",
       "  ' r'],\n",
       " ['using molecular hydrogen as its energy source we re looking to see if we can take captured co two which can easily be piped to sites convert that co two back into fuel to drive this process so',\n",
       "  ' r'],\n",
       " ['and we ve been trying to just see if we can come up with an even smaller genome',\n",
       "  ' r'],\n",
       " ['into two sequence siftables arrange them into a series', ' r'],\n",
       " ['they really loved it and one of the interesting things about this kind of application is that you don t have to give people many instructions all you have to say is make words and they know exactly what to do',\n",
       "  ' r r r r r r'],\n",
       " ['and i said no well it turns out you take each of the seventeen varieties in the night of a full moon and it sings to you in a different key now that s not going to get you a ph d at harvard but it s a lot more interesting than counting stamens',\n",
       "  ' r r'],\n",
       " ['to show you what i think is really the the punchline behind this this technology the photosynth technology and it s not necessarily so apparent from looking at the environments that we ve put up on the website we we had to worry about the lawyers and so on',\n",
       "  ' r r r r'],\n",
       " ['take a good long look this is today s clean coal technology', 'r r r r'],\n",
       " ['we added a few extra genes so we could select for this chromosome we digested it with enzymes to kill all the proteins and it was pretty stunning when we put this in the cell and you ll appreciate our very sophisticated graphics here',\n",
       "  ' r r'],\n",
       " ['now this is an interactive cartoon application and we wanted to build a learning tool for language learners and this is felix actually',\n",
       "  'r r r'],\n",
       " ['morphologically unrelated plants that when combined in this way created a kind of biochemical version of the whole being greater than the sum of the parts well we use that great euphemism trial and error which is exposed to be meaningless but you ask the indians and they say the plants talk to us well what does that mean this tribe the cofan has seventeen varieties of ayahuasca all of which they distinguish a great distance in the forest all of which are referable to our eye',\n",
       "  ' r'],\n",
       " ['you wouldn t all know what this was about the truth is more like this',\n",
       "  ' r'],\n",
       " ['single consumer electronics store there are that many stereo systems we can construct six and a half million',\n",
       "  ' r']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you know one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways who still feel their past in the wind touch it in stones polished by rain taste it in the bitter leaves of plants just to know that jaguar shamans still journey beyond the milky way or the'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2str(df.iloc[1].labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>nb_frames</th>\n",
       "      <th>labels</th>\n",
       "      <th>offset</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WadeDavis_2003.sph.wav-12.51</td>\n",
       "      <td>2202</td>\n",
       "      <td>[26, 16, 22, 1, 12, 15, 16, 24, 1, 16, 15, 6, ...</td>\n",
       "      <td>12.51</td>\n",
       "      <td>22.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WadeDavis_2003.sph.wav-34.52</td>\n",
       "      <td>643</td>\n",
       "      <td>[21, 9, 6, 1, 14, 26, 21, 9, 20, 1, 16, 7, 1, ...</td>\n",
       "      <td>34.52</td>\n",
       "      <td>6.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WadeDavis_2003.sph.wav-40.94</td>\n",
       "      <td>2159</td>\n",
       "      <td>[3, 22, 5, 5, 9, 10, 20, 21, 20, 1, 20, 21, 10...</td>\n",
       "      <td>40.94</td>\n",
       "      <td>21.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WadeDavis_2003.sph.wav-62.52</td>\n",
       "      <td>1291</td>\n",
       "      <td>[2, 15, 5, 1, 16, 7, 1, 4, 16, 22, 19, 20, 6, ...</td>\n",
       "      <td>62.52</td>\n",
       "      <td>12.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WadeDavis_2003.sph.wav-75.42</td>\n",
       "      <td>1481</td>\n",
       "      <td>[24, 6, 1, 2, 13, 13, 1, 5, 2, 15, 4, 6, 1, 24...</td>\n",
       "      <td>75.42</td>\n",
       "      <td>14.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WadeDavis_2003.sph.wav-90.22</td>\n",
       "      <td>860</td>\n",
       "      <td>[16, 19, 1, 21, 9, 6, 1, 24, 2, 19, 19, 10, 16...</td>\n",
       "      <td>90.22</td>\n",
       "      <td>8.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WadeDavis_2003.sph.wav-98.81</td>\n",
       "      <td>884</td>\n",
       "      <td>[16, 19, 1, 2, 1, 4, 2, 1, 4, 2, 19, 2, 23, 2,...</td>\n",
       "      <td>98.81</td>\n",
       "      <td>8.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>WadeDavis_2003.sph.wav-107.65</td>\n",
       "      <td>1567</td>\n",
       "      <td>[16, 19, 1, 10, 15, 5, 6, 6, 5, 1, 2, 1, 26, 2...</td>\n",
       "      <td>107.65</td>\n",
       "      <td>15.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WadeDavis_2003.sph.wav-123.31</td>\n",
       "      <td>3993</td>\n",
       "      <td>[15, 16, 24, 1, 21, 16, 8, 6, 21, 9, 6, 19, 1,...</td>\n",
       "      <td>123.31</td>\n",
       "      <td>39.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WadeDavis_2003.sph.wav-163.23</td>\n",
       "      <td>1682</td>\n",
       "      <td>[3, 10, 16, 20, 17, 9, 6, 19, 6, 1, 9, 2, 20, ...</td>\n",
       "      <td>163.23</td>\n",
       "      <td>16.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>WadeDavis_2003.sph.wav-180.04</td>\n",
       "      <td>1238</td>\n",
       "      <td>[3, 10, 16, 13, 16, 8, 10, 4, 2, 13, 1, 5, 10,...</td>\n",
       "      <td>180.04</td>\n",
       "      <td>12.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>WadeDavis_2003.sph.wav-192.41</td>\n",
       "      <td>2182</td>\n",
       "      <td>[21, 9, 6, 19, 6, 1, 24, 6, 19, 6, 1, 20, 10, ...</td>\n",
       "      <td>192.41</td>\n",
       "      <td>21.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>WadeDavis_2003.sph.wav-214.22</td>\n",
       "      <td>2426</td>\n",
       "      <td>[2, 15, 5, 1, 16, 7, 1, 21, 9, 16, 20, 6, 1, 2...</td>\n",
       "      <td>214.22</td>\n",
       "      <td>24.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>WadeDavis_2003.sph.wav-238.47</td>\n",
       "      <td>1031</td>\n",
       "      <td>[4, 9, 10, 13, 5, 19, 6, 15, 1, 2, 15, 5, 1, 2...</td>\n",
       "      <td>238.47</td>\n",
       "      <td>10.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>WadeDavis_2003.sph.wav-248.77</td>\n",
       "      <td>1153</td>\n",
       "      <td>[16, 7, 1, 2, 15, 1, 2, 15, 4, 10, 6, 15, 21, ...</td>\n",
       "      <td>248.77</td>\n",
       "      <td>11.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>WadeDavis_2003.sph.wav-260.29</td>\n",
       "      <td>997</td>\n",
       "      <td>[12, 16, 8, 10, 1, 2, 15, 5, 1, 26, 16, 22, 1,...</td>\n",
       "      <td>260.29</td>\n",
       "      <td>9.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>WadeDavis_2003.sph.wav-270.25</td>\n",
       "      <td>836</td>\n",
       "      <td>[16, 15, 1, 2, 1, 11, 16, 22, 19, 15, 6, 26, 1...</td>\n",
       "      <td>270.25</td>\n",
       "      <td>8.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>WadeDavis_2003.sph.wav-278.61</td>\n",
       "      <td>699</td>\n",
       "      <td>[15, 16, 24, 1, 21, 9, 6, 19, 6, 1, 2, 19, 6, ...</td>\n",
       "      <td>278.61</td>\n",
       "      <td>6.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>WadeDavis_2003.sph.wav-285.60</td>\n",
       "      <td>1003</td>\n",
       "      <td>[7, 16, 19, 8, 6, 21, 1, 21, 9, 2, 21, 1, 24, ...</td>\n",
       "      <td>285.60</td>\n",
       "      <td>10.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>WadeDavis_2003.sph.wav-295.62</td>\n",
       "      <td>955</td>\n",
       "      <td>[17, 6, 16, 17, 13, 6, 1, 16, 7, 1, 21, 9, 6, ...</td>\n",
       "      <td>295.62</td>\n",
       "      <td>9.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>WadeDavis_2003.sph.wav-305.16</td>\n",
       "      <td>2197</td>\n",
       "      <td>[5, 16, 1, 15, 16, 21, 1, 5, 10, 20, 21, 10, 1...</td>\n",
       "      <td>305.16</td>\n",
       "      <td>21.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>WadeDavis_2003.sph.wav-327.12</td>\n",
       "      <td>711</td>\n",
       "      <td>[3, 6, 4, 2, 22, 20, 6, 1, 16, 7, 1, 10, 15, 2...</td>\n",
       "      <td>327.12</td>\n",
       "      <td>7.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>WadeDavis_2003.sph.wav-334.22</td>\n",
       "      <td>2652</td>\n",
       "      <td>[16, 19, 1, 16, 15, 6, 1, 16, 7, 1, 21, 9, 6, ...</td>\n",
       "      <td>334.22</td>\n",
       "      <td>26.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>WadeDavis_2003.sph.wav-360.73</td>\n",
       "      <td>2706</td>\n",
       "      <td>[21, 19, 10, 6, 5, 1, 21, 16, 1, 13, 16, 16, 1...</td>\n",
       "      <td>360.73</td>\n",
       "      <td>27.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>WadeDavis_2003.sph.wav-387.78</td>\n",
       "      <td>2565</td>\n",
       "      <td>[3, 22, 21, 1, 2, 21, 1, 21, 9, 6, 1, 20, 2, 1...</td>\n",
       "      <td>387.78</td>\n",
       "      <td>25.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>WadeDavis_2003.sph.wav-413.42</td>\n",
       "      <td>2029</td>\n",
       "      <td>[10, 15, 1, 16, 19, 5, 6, 19, 1, 21, 16, 1, 14...</td>\n",
       "      <td>413.42</td>\n",
       "      <td>20.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>WadeDavis_2003.sph.wav-433.70</td>\n",
       "      <td>669</td>\n",
       "      <td>[2, 20, 20, 22, 14, 17, 21, 10, 16, 15, 1, 3, ...</td>\n",
       "      <td>433.70</td>\n",
       "      <td>6.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>WadeDavis_2003.sph.wav-440.38</td>\n",
       "      <td>2300</td>\n",
       "      <td>[17, 19, 16, 7, 16, 22, 15, 5, 1, 19, 6, 13, 1...</td>\n",
       "      <td>440.38</td>\n",
       "      <td>22.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>WadeDavis_2003.sph.wav-463.38</td>\n",
       "      <td>1085</td>\n",
       "      <td>[26, 16, 22, 1, 24, 9, 10, 21, 6, 1, 17, 6, 16...</td>\n",
       "      <td>463.38</td>\n",
       "      <td>10.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>WadeDavis_2003.sph.wav-474.22</td>\n",
       "      <td>848</td>\n",
       "      <td>[5, 6, 14, 16, 15, 20, 21, 19, 2, 21, 10, 16, ...</td>\n",
       "      <td>474.22</td>\n",
       "      <td>8.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>WadeDavis_2003.sph.wav-482.69</td>\n",
       "      <td>973</td>\n",
       "      <td>[14, 10, 15, 5, 1, 21, 16, 1, 2, 7, 7, 6, 4, 2...</td>\n",
       "      <td>482.69</td>\n",
       "      <td>9.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>WadeDavis_2003.sph.wav-492.41</td>\n",
       "      <td>1571</td>\n",
       "      <td>[16, 7, 1, 21, 9, 6, 1, 20, 10, 6, 19, 19, 2, ...</td>\n",
       "      <td>492.41</td>\n",
       "      <td>15.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>WadeDavis_2003.sph.wav-508.11</td>\n",
       "      <td>1888</td>\n",
       "      <td>[10, 15, 1, 2, 1, 3, 13, 16, 16, 5, 20, 21, 2,...</td>\n",
       "      <td>508.11</td>\n",
       "      <td>18.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>WadeDavis_2003.sph.wav-526.98</td>\n",
       "      <td>1247</td>\n",
       "      <td>[7, 16, 19, 1, 6, 10, 8, 9, 21, 6, 6, 15, 1, 2...</td>\n",
       "      <td>526.98</td>\n",
       "      <td>12.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>WadeDavis_2003.sph.wav-539.44</td>\n",
       "      <td>3002</td>\n",
       "      <td>[2, 19, 6, 1, 10, 15, 4, 22, 13, 21, 22, 19, 2...</td>\n",
       "      <td>539.44</td>\n",
       "      <td>30.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         filename nb_frames  \\\n",
       "0    WadeDavis_2003.sph.wav-12.51      2202   \n",
       "1    WadeDavis_2003.sph.wav-34.52       643   \n",
       "2    WadeDavis_2003.sph.wav-40.94      2159   \n",
       "3    WadeDavis_2003.sph.wav-62.52      1291   \n",
       "4    WadeDavis_2003.sph.wav-75.42      1481   \n",
       "5    WadeDavis_2003.sph.wav-90.22       860   \n",
       "6    WadeDavis_2003.sph.wav-98.81       884   \n",
       "7   WadeDavis_2003.sph.wav-107.65      1567   \n",
       "8   WadeDavis_2003.sph.wav-123.31      3993   \n",
       "9   WadeDavis_2003.sph.wav-163.23      1682   \n",
       "10  WadeDavis_2003.sph.wav-180.04      1238   \n",
       "11  WadeDavis_2003.sph.wav-192.41      2182   \n",
       "12  WadeDavis_2003.sph.wav-214.22      2426   \n",
       "13  WadeDavis_2003.sph.wav-238.47      1031   \n",
       "14  WadeDavis_2003.sph.wav-248.77      1153   \n",
       "15  WadeDavis_2003.sph.wav-260.29       997   \n",
       "16  WadeDavis_2003.sph.wav-270.25       836   \n",
       "17  WadeDavis_2003.sph.wav-278.61       699   \n",
       "18  WadeDavis_2003.sph.wav-285.60      1003   \n",
       "19  WadeDavis_2003.sph.wav-295.62       955   \n",
       "20  WadeDavis_2003.sph.wav-305.16      2197   \n",
       "21  WadeDavis_2003.sph.wav-327.12       711   \n",
       "22  WadeDavis_2003.sph.wav-334.22      2652   \n",
       "23  WadeDavis_2003.sph.wav-360.73      2706   \n",
       "24  WadeDavis_2003.sph.wav-387.78      2565   \n",
       "25  WadeDavis_2003.sph.wav-413.42      2029   \n",
       "26  WadeDavis_2003.sph.wav-433.70       669   \n",
       "27  WadeDavis_2003.sph.wav-440.38      2300   \n",
       "28  WadeDavis_2003.sph.wav-463.38      1085   \n",
       "29  WadeDavis_2003.sph.wav-474.22       848   \n",
       "30  WadeDavis_2003.sph.wav-482.69       973   \n",
       "31  WadeDavis_2003.sph.wav-492.41      1571   \n",
       "32  WadeDavis_2003.sph.wav-508.11      1888   \n",
       "33  WadeDavis_2003.sph.wav-526.98      1247   \n",
       "34  WadeDavis_2003.sph.wav-539.44      3002   \n",
       "\n",
       "                                               labels  offset  duration  \n",
       "0   [26, 16, 22, 1, 12, 15, 16, 24, 1, 16, 15, 6, ...   12.51     22.01  \n",
       "1   [21, 9, 6, 1, 14, 26, 21, 9, 20, 1, 16, 7, 1, ...   34.52      6.42  \n",
       "2   [3, 22, 5, 5, 9, 10, 20, 21, 20, 1, 20, 21, 10...   40.94     21.58  \n",
       "3   [2, 15, 5, 1, 16, 7, 1, 4, 16, 22, 19, 20, 6, ...   62.52     12.90  \n",
       "4   [24, 6, 1, 2, 13, 13, 1, 5, 2, 15, 4, 6, 1, 24...   75.42     14.80  \n",
       "5   [16, 19, 1, 21, 9, 6, 1, 24, 2, 19, 19, 10, 16...   90.22      8.59  \n",
       "6   [16, 19, 1, 2, 1, 4, 2, 1, 4, 2, 19, 2, 23, 2,...   98.81      8.83  \n",
       "7   [16, 19, 1, 10, 15, 5, 6, 6, 5, 1, 2, 1, 26, 2...  107.65     15.66  \n",
       "8   [15, 16, 24, 1, 21, 16, 8, 6, 21, 9, 6, 19, 1,...  123.31     39.92  \n",
       "9   [3, 10, 16, 20, 17, 9, 6, 19, 6, 1, 9, 2, 20, ...  163.23     16.81  \n",
       "10  [3, 10, 16, 13, 16, 8, 10, 4, 2, 13, 1, 5, 10,...  180.04     12.37  \n",
       "11  [21, 9, 6, 19, 6, 1, 24, 6, 19, 6, 1, 20, 10, ...  192.41     21.81  \n",
       "12  [2, 15, 5, 1, 16, 7, 1, 21, 9, 16, 20, 6, 1, 2...  214.22     24.25  \n",
       "13  [4, 9, 10, 13, 5, 19, 6, 15, 1, 2, 15, 5, 1, 2...  238.47     10.30  \n",
       "14  [16, 7, 1, 2, 15, 1, 2, 15, 4, 10, 6, 15, 21, ...  248.77     11.52  \n",
       "15  [12, 16, 8, 10, 1, 2, 15, 5, 1, 26, 16, 22, 1,...  260.29      9.96  \n",
       "16  [16, 15, 1, 2, 1, 11, 16, 22, 19, 15, 6, 26, 1...  270.25      8.35  \n",
       "17  [15, 16, 24, 1, 21, 9, 6, 19, 6, 1, 2, 19, 6, ...  278.61      6.98  \n",
       "18  [7, 16, 19, 8, 6, 21, 1, 21, 9, 2, 21, 1, 24, ...  285.60     10.02  \n",
       "19  [17, 6, 16, 17, 13, 6, 1, 16, 7, 1, 21, 9, 6, ...  295.62      9.54  \n",
       "20  [5, 16, 1, 15, 16, 21, 1, 5, 10, 20, 21, 10, 1...  305.16     21.96  \n",
       "21  [3, 6, 4, 2, 22, 20, 6, 1, 16, 7, 1, 10, 15, 2...  327.12      7.10  \n",
       "22  [16, 19, 1, 16, 15, 6, 1, 16, 7, 1, 21, 9, 6, ...  334.22     26.51  \n",
       "23  [21, 19, 10, 6, 5, 1, 21, 16, 1, 13, 16, 16, 1...  360.73     27.05  \n",
       "24  [3, 22, 21, 1, 2, 21, 1, 21, 9, 6, 1, 20, 2, 1...  387.78     25.64  \n",
       "25  [10, 15, 1, 16, 19, 5, 6, 19, 1, 21, 16, 1, 14...  413.42     20.28  \n",
       "26  [2, 20, 20, 22, 14, 17, 21, 10, 16, 15, 1, 3, ...  433.70      6.68  \n",
       "27  [17, 19, 16, 7, 16, 22, 15, 5, 1, 19, 6, 13, 1...  440.38     22.99  \n",
       "28  [26, 16, 22, 1, 24, 9, 10, 21, 6, 1, 17, 6, 16...  463.38     10.84  \n",
       "29  [5, 6, 14, 16, 15, 20, 21, 19, 2, 21, 10, 16, ...  474.22      8.47  \n",
       "30  [14, 10, 15, 5, 1, 21, 16, 1, 2, 7, 7, 6, 4, 2...  482.69      9.72  \n",
       "31  [16, 7, 1, 21, 9, 6, 1, 20, 10, 6, 19, 19, 2, ...  492.41     15.70  \n",
       "32  [10, 15, 1, 2, 1, 3, 13, 16, 16, 5, 20, 21, 2,...  508.11     18.87  \n",
       "33  [7, 16, 19, 1, 6, 10, 8, 9, 21, 6, 6, 15, 1, 2...  526.98     12.46  \n",
       "34  [2, 19, 6, 1, 10, 15, 4, 22, 13, 21, 22, 19, 2...  539.44     30.01  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/aimlx/kws/Notebooks'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32512"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file = \"/aimlx/Datasets/tedlium/TEDLIUM_release1/dev/sph/AlGore_2009.sph.wav\"\n",
    "os.system(\"aplay \" + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
