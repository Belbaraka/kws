{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from python_speech_features import mfcc\n",
    "import scipy.io.wavfile as wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.abspath(\"../Datasets/google_dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['up', 'down']\n",
    "non_keywords = ['happy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get keyword filename of audio samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "\n",
    "for w in (keywords + non_keywords):\n",
    "    current_path = os.path.join(path, w) \n",
    "    for _, _, files in os.walk(current_path):\n",
    "        for file in files:\n",
    "            filenames.append(os.path.join(current_path, file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WAVS_PER_CLASS = 2**27 - 1  # ~134M\n",
    "\n",
    "def which_set(filename, validation_percentage, testing_percentage):\n",
    "    \"\"\"Determines which data partition the file should belong to.\n",
    "\n",
    "    We want to keep files in the same training, validation, or testing sets even\n",
    "    if new ones are added over time. This makes it less likely that testing\n",
    "    samples will accidentally be reused in training when long runs are restarted\n",
    "    for example. To keep this stability, a hash of the filename is taken and used\n",
    "    to determine which set it should belong to. This determination only depends on\n",
    "    the name and the set proportions, so it won't change as other files are added.\n",
    "\n",
    "    It's also useful to associate particular files as related (for example words\n",
    "    spoken by the same person), so anything after '_nohash_' in a filename is\n",
    "    ignored for set determination. This ensures that 'bobby_nohash_0.wav' and\n",
    "    'bobby_nohash_1.wav' are always in the same set, for example.\n",
    "\n",
    "    Args:\n",
    "    filename: File path of the data sample.\n",
    "    validation_percentage: How much of the data set to use for validation.\n",
    "    testing_percentage: How much of the data set to use for testing.\n",
    "\n",
    "    Returns:\n",
    "    String, one of 'training', 'validation', or 'testing'.\n",
    "    \"\"\"\n",
    "    base_name = os.path.basename(filename)\n",
    "    # We want to ignore anything after '_nohash_' in the file name when\n",
    "    # deciding which set to put a wav in, so the data set creator has a way of\n",
    "    # grouping wavs that are close variations of each other.\n",
    "    hash_name = re.sub(r'_nohash_.*$', '', base_name).encode('utf-8')\n",
    "    # This looks a bit magical, but we need to decide whether this file should\n",
    "    # go into the training, testing, or validation sets, and we want to keep\n",
    "    # existing files in the same set even if more files are subsequently\n",
    "    # added.\n",
    "    # To do that, we need a stable way of deciding based on just the file name\n",
    "    # itself, so we do a hash of that and then use that to generate a\n",
    "    # probability value that we use to assign it.\n",
    "    hash_name_hashed = hashlib.sha1(hash_name).hexdigest()\n",
    "    percentage_hash = ((int(hash_name_hashed, 16) %\n",
    "                      (MAX_NUM_WAVS_PER_CLASS + 1)) *\n",
    "                     (100.0 / MAX_NUM_WAVS_PER_CLASS))\n",
    "    if percentage_hash < validation_percentage:\n",
    "        result = 'validation'\n",
    "    elif percentage_hash < (testing_percentage + validation_percentage):\n",
    "        result = 'testing'\n",
    "    else:\n",
    "        result = 'training'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'testing'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "which_set(random.choice(filenames), validation_percentage=10, testing_percentage=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mfcc(signal, num_features=40): \n",
    "\n",
    "    features = mfcc(signal, samplerate=16000, winlen=0.030, winstep=0.01, numcep=num_features, \n",
    "                         lowfreq=20, highfreq=4000, appendEnergy=False, nfilt=num_features)\n",
    "    #features = np.mean(features, axis=0).reshape(1,-1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, sig = wav.read(random.choice(filenames))\n",
    "feats = compute_mfcc(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98, 40)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sets(filenames, validation_percentage=10, testing_percentage=10):\n",
    "    '''\n",
    "    each data sample in the tuple (features, label)\n",
    "    '''\n",
    "    \n",
    "    non_keywords_label = len(keywords)\n",
    "\n",
    "    training, validation, testing = [], [], []\n",
    "    min_nb_frames = 98\n",
    "    for filename in tqdm(filenames):\n",
    "        _, signal = wav.read(filename)\n",
    "        feats = compute_mfcc(signal)\n",
    "        \n",
    "        if feats.shape[0] < min_nb_frames:\n",
    "            continue\n",
    "        kw = filename.split('/')[-2]\n",
    "        if kw in keywords:\n",
    "            label = keywords.index(kw)\n",
    "        else:\n",
    "            label = non_keywords_label\n",
    "            \n",
    "        grp = which_set(filename, validation_percentage, testing_percentage)\n",
    "        \n",
    "        if grp is 'training':\n",
    "            training.append((feats, label))\n",
    "        elif grp is 'validation':\n",
    "            validation.append((feats, label))\n",
    "        else:\n",
    "            testing.append((feats, label))\n",
    "    return training, validation, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6476/6476 [00:20<00:00, 322.22it/s]\n"
     ]
    }
   ],
   "source": [
    "training, validation, testing = generate_sets(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(grp):\n",
    "    X, y = zip(*grp)\n",
    "    X = list(map(lambda x: x.reshape(98, 40, 1), X))\n",
    "    return np.array(X).reshape(-1, 98, 40, 1), np.array(y).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = get_X_y(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization, Conv2D, AveragePooling2D, Dense, Flatten\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(45, kernel_size=(3,3), activation='relu', input_shape=(98, 40, 1), \n",
    "                     data_format='channels_last', padding='same')) \n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Conv2D(45, kernel_size=(3,3), activation='relu', padding='same',dilation_rate=2))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=len(keywords) + 1, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_X_y(training)\n",
    "X_validation, y_validation = get_X_y(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_validation = to_categorical(y_train), to_categorical(y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4484 samples, validate on 634 samples\n",
      "Epoch 1/2\n",
      "4484/4484 [==============================] - 170s 38ms/step - loss: 1.0417 - acc: 0.7806 - val_loss: 0.8464 - val_acc: 0.8533\n",
      "Epoch 2/2\n",
      "4484/4484 [==============================] - 175s 39ms/step - loss: 0.3274 - acc: 0.9222 - val_loss: 0.7630 - val_acc: 0.8738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1045e87b8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.fit(X_train, y_train, batch_size=20, epochs=2, verbose=1, validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = get_X_y(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646/646 [==============================] - 8s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8637770897832817"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.804953560371517"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
